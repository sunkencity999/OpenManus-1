"""
Improved Manus agent with enhanced reasoning capabilities.
"""

import json
import os
import re
import time
import traceback  # For detailed error logging in python_execute
from datetime import datetime  # Added for timestamps
from pathlib import Path  # Added for path manipulation
from typing import Any, Dict, List, Optional, Set
from urllib.parse import parse_qs, unquote, urljoin, urlparse  # Added for URL parsing

import requests  # Added for Ollama
from bs4 import BeautifulSoup  # Added for parsing
from pydantic import Field, model_validator

# Assuming these imports point to the correct modules in your project structure
from app.agent.browser import BrowserContextHelper
from app.agent.browser_navigator import BrowserNavigator
from app.agent.memory_manager import (
    ToolUsageTracker,
)  # ConversationMemory removed if PersistentMemory replaces its direct use
from app.agent.persistent_memory import PersistentMemory  # Using PersistentMemory
from app.agent.task_analyzer import TaskAnalyzer
from app.agent.task_completer import TaskCompleter
from app.agent.toolcall import ToolCallAgent
from app.agent.url_detector import URLDetector
from app.config import config  # Import config singleton
from app.logger import logger
from app.prompt.improved_manus import IMPROVED_NEXT_STEP_PROMPT, IMPROVED_SYSTEM_PROMPT
from app.schema import Message, ToolCall
from app.tool import Terminate, ToolCollection
from app.tool.browser_use_tool import BrowserUseTool
from app.tool.mcp import MCPClients, MCPClientTool
from app.tool.python_execute import PythonExecute
from app.tool.smart_ask_human import SmartAskHuman
from app.tool.str_replace_editor import StrReplaceEditor


class ImprovedManus(ToolCallAgent):
    """
    An improved version of the Manus agent with better reasoning capabilities.
    Focuses on minimizing unnecessary questions and making better decisions about
    when user input is truly required.
    """

    name: str = "ImprovedManus"
    description: str = (
        "A versatile agent that can solve various tasks using multiple tools including MCP-based tools, with improved reasoning"
    )

    system_prompt: str = IMPROVED_SYSTEM_PROMPT.format(directory=config.workspace_root)
    next_step_prompt: str = IMPROVED_NEXT_STEP_PROMPT

    max_observe: int = 15000  # Increased observation length for web content
    max_steps: int = 25  # Increased max steps slightly

    # MCP clients for remote tool access
    mcp_clients: MCPClients = Field(default_factory=MCPClients)

    # Add general-purpose tools to the tool collection, using SmartAskHuman instead of AskHuman
    available_tools: ToolCollection = Field(
        default_factory=lambda: ToolCollection(
            PythonExecute(),
            BrowserUseTool(),
            StrReplaceEditor(),
            SmartAskHuman(),  # SmartAskHuman should ideally use PersistentMemory
            Terminate(),
        )
    )

    special_tool_names: list[str] = Field(default_factory=lambda: [Terminate().name])
    browser_context_helper: Optional[BrowserContextHelper] = None

    # Track connected MCP servers
    connected_servers: Dict[str, str] = Field(
        default_factory=dict
    )  # server_id -> url/command
    _initialized: bool = False

    # New components for improved reasoning
    conversation_memory: PersistentMemory = None  # Using PersistentMemory
    browser_navigator: BrowserNavigator = None
    url_detector: URLDetector = None
    task_completer: TaskCompleter = None
    task_analyzer: Optional[TaskAnalyzer] = None

    # Memory database path - if None, will use the default platform-specific path
    memory_db_path: Optional[str] = Field(default=None)

    # Tracking attributes
    browser_used: bool = False
    visited_urls: Set[str] = Field(default_factory=set)
    current_task: str = ""
    task_completed: bool = False

    # Memory management components
    tool_tracker: ToolUsageTracker = Field(default_factory=ToolUsageTracker)

    # Track URLs mentioned in the conversation
    mentioned_urls: Set[str] = Field(default_factory=set)

    # Flag to prevent repeated tool calls
    _prevent_repeated_questions: bool = True

    @model_validator(mode="after")
    def initialize_helper(self) -> "ImprovedManus":
        """Initialize basic components synchronously."""
        self.browser_context_helper = BrowserContextHelper(self)

        # Use PersistentMemory
        if self.conversation_memory is None:  # Initialize only if not already set
            if self.memory_db_path is not None:
                # If a specific path is provided, use it, relative to workspace
                memory_path = Path(config.workspace_root) / self.memory_db_path
                memory_path.parent.mkdir(
                    parents=True, exist_ok=True
                )  # Ensure directory exists
                self.conversation_memory = PersistentMemory(db_path=str(memory_path))
                logger.info(
                    f"Initialized PersistentMemory with custom path: {memory_path}"
                )
            else:
                # Otherwise use the default platform-specific path
                self.conversation_memory = (
                    PersistentMemory()
                )  # Default path handled by PersistentMemory class
                logger.info(
                    f"Initialized PersistentMemory with default path: {self.conversation_memory.db_path}"
                )

        self.browser_navigator = BrowserNavigator()
        self.url_detector = URLDetector()
        self.task_completer = TaskCompleter()
        self.task_analyzer = TaskAnalyzer(llm=self.llm)

        # Set the context_manager for SmartAskHuman tool
        smart_ask_tool = self.available_tools.get_tool("ask_human")
        if isinstance(smart_ask_tool, SmartAskHuman):
            smart_ask_tool.context_manager = self  # SmartAskHuman expects a context_manager

        return self

    @classmethod
    async def create(cls, **kwargs) -> "ImprovedManus":
        """Factory method to create and properly initialize an ImprovedManus instance."""
        instance = cls(**kwargs)
        # Initialization including MCP servers is now handled after Pydantic validation
        # We ensure _initialized is True after successful post-validation setup
        # await instance.initialize_mcp_servers() # Called from think if not initialized
        instance._initialized = False  # Will be set true in think or after MCP init
        return instance

    async def initialize_mcp_servers(self) -> None:
        """Initialize connections to configured MCP servers."""
        if self._initialized:  # Avoid re-initialization
            return

        cfg = config  # Use the config singleton
        if (
            not hasattr(cfg, "mcp_config")
            or not cfg.mcp_config
            or not hasattr(cfg.mcp_config, "servers")
        ):
            logger.warning(
                "MCP configuration not found or empty. Skipping MCP server initialization."
            )
            self._initialized = True  # Mark as initialized even if no MCP servers
            return

        logger.info("Initializing MCP server connections...")
        connect_tasks = []
        for server_id, server_config in cfg.mcp_config.servers.items():
            try:
                if server_config.type == "sse" and server_config.url:
                    connect_tasks.append(
                        self.connect_mcp_server(server_config.url, server_id)
                    )
                elif server_config.type == "stdio" and server_config.command:
                    connect_tasks.append(
                        self.connect_mcp_server(
                            server_config.command,
                            server_id,
                            use_stdio=True,
                            stdio_args=server_config.args,
                        )
                    )
            except Exception as e:
                logger.error(
                    f"Failed to prepare connection for MCP server {server_id}: {e}"
                )

        if connect_tasks:
            await asyncio.gather(*connect_tasks)  # Connect concurrently
            logger.info(f"Attempted connection to {len(connect_tasks)} MCP servers.")
        else:
            logger.info("No MCP servers configured to connect.")

        self._initialized = True

    async def connect_mcp_server(
        self,
        server_endpoint: str,  # Renamed from server_url to endpoint (can be command or url)
        server_id: str = "",
        use_stdio: bool = False,
        stdio_args: List[str] = None,
    ) -> None:
        """Connect to an MCP server and add its tools."""
        server_id = server_id or server_endpoint  # Ensure server_id has a value
        if server_id in self.connected_servers:
            logger.warning(f"Already connected to MCP server {server_id}. Skipping.")
            return

        logger.info(
            f"Connecting to MCP server {server_id} ({'stdio' if use_stdio else 'sse'})..."
        )
        try:
            if use_stdio:
                await self.mcp_clients.connect_stdio(
                    server_endpoint, stdio_args or [], server_id
                )
                self.connected_servers[server_id] = f"stdio:{server_endpoint}"
                logger.info(
                    f"Connected to MCP server {server_id} using command {server_endpoint}"
                )
            else:
                await self.mcp_clients.connect_sse(server_endpoint, server_id)
                self.connected_servers[server_id] = f"sse:{server_endpoint}"
                logger.info(f"Connected to MCP server {server_id} at {server_endpoint}")

            # Update available tools with only the new tools from this server
            new_tools = [
                tool
                for tool in self.mcp_clients.tools
                if getattr(tool, "server_id", None) == server_id
            ]
            if new_tools:
                self.available_tools.add_tools(*new_tools)
                logger.info(
                    f"Added {len(new_tools)} tools from MCP server {server_id}: {[t.name for t in new_tools]}"
                )
            else:
                logger.warning(
                    f"No new tools found for MCP server {server_id} after connection."
                )

        except Exception as e:
            logger.error(
                f"Failed to connect to MCP server {server_id} ({server_endpoint}): {e}",
                exc_info=True,
            )
            # Clean up if connection failed partially
            self.connected_servers.pop(server_id, None)

    async def disconnect_mcp_server(self, server_id: str) -> None:
        """Disconnect from an MCP server and remove its tools."""
        if not server_id or server_id not in self.connected_servers:
            logger.warning(
                f"Disconnect MCP: Server ID '{server_id}' not found or not connected."
            )
            return

        logger.info(f"Disconnecting from MCP server {server_id}...")
        try:
            await self.mcp_clients.disconnect(server_id)
            self.connected_servers.pop(server_id, None)

            # Rebuild available tools list more carefully
            current_tools = self.available_tools.tools
            updated_tools = []
            removed_count = 0
            for tool in current_tools:
                if (
                    isinstance(tool, MCPClientTool)
                    and getattr(tool, "server_id", None) == server_id
                ):
                    removed_count += 1
                    continue  # Skip tools from the disconnected server
                updated_tools.append(tool)

            self.available_tools = ToolCollection(
                *updated_tools
            )  # Recreate with filtered list
            logger.info(
                f"Disconnected from MCP server {server_id}. Removed {removed_count} tools."
            )

        except Exception as e:
            logger.error(
                f"Error disconnecting from MCP server {server_id}: {e}", exc_info=True
            )

    async def cleanup(self):
        """Clean up ImprovedManus agent resources."""
        logger.info("Cleaning up ImprovedManus agent...")
        if self.browser_context_helper:
            await self.browser_context_helper.cleanup_browser()
            logger.info("Browser context cleaned up.")

        # Disconnect from all MCP servers
        if self.connected_servers:
            server_ids_to_disconnect = list(self.connected_servers.keys())
            logger.info(
                f"Disconnecting from {len(server_ids_to_disconnect)} MCP servers..."
            )
            disconnect_tasks = [
                self.disconnect_mcp_server(sid) for sid in server_ids_to_disconnect
            ]
            await asyncio.gather(*disconnect_tasks)
            logger.info("MCP servers disconnected.")

        # Close persistent memory connection
        if self.conversation_memory:
            try:
                await self.conversation_memory.close()
                logger.info("Persistent memory connection closed.")
            except Exception as e:
                logger.error(f"Error closing persistent memory: {e}")

        self._initialized = False
        logger.info("ImprovedManus cleanup complete.")

    async def analyze_new_task(self, task: str) -> None:
        """Analyze a new task to determine required information and steps."""
        logger.info(f"Analyzing new task: '{task[:100]}...'")
        # Reset components for new task
        if self.conversation_memory:
            self.conversation_memory.reset()  # Reset persistent memory context for new task
            logger.info("Persistent memory reset for new task.")
        else:
            logger.warning(
                "Conversation memory not initialized during analyze_new_task."
            )

        self.tool_tracker.reset()
        self.url_detector.extract_urls(task)
        self.mentioned_urls = set(
            self.url_detector.mentioned_urls
        )  # Initialize from task

        # Initialize or reset task completer
        self.task_completer = TaskCompleter()  # Re-initialize
        self.task_completer.analyze_task(task)
        logger.info(
            f"Task analyzed. Type: {self.task_completer.task_type}, Missing info: {self.task_completer.get_missing_info()}"
        )

        # Store the current task description
        self.current_task = task
        self.task_completed = False
        self.browser_used = False
        self.visited_urls = set()
        self.step_count = 0  # Reset step counter

        # Reset SmartAskHuman state
        smart_ask_tool = self.available_tools.get_tool("ask_human")
        if isinstance(smart_ask_tool, SmartAskHuman):
            smart_ask_tool.reset_for_new_task()

        # Perform task analysis using TaskAnalyzer
        if self.task_analyzer:
            logger.info("Running TaskAnalyzer...")
            try:
                plan = await self.task_analyzer.analyze_task(task)
                if plan and plan.steps:
                    required_context = plan.all_required_context()
                    if required_context:
                        self.context_manager.set_required_context(
                            list(required_context)
                        )
                        logger.info(
                            f"Task analysis set required context: {required_context}"
                        )

                    logger.info(f"ðŸ“‹ Task plan created with {len(plan.steps)} steps:")
                    for i, step in enumerate(plan.steps):
                        log_msg = f"  Step {i+1}: {step.description}"
                        if step.required_context:
                            log_msg += (
                                f" (Requires: {', '.join(step.required_context)})"
                            )
                        logger.info(log_msg)
                else:
                    logger.info("Task analysis did not produce a plan.")

                # Create navigation plan if URLs are present
                if self.mentioned_urls:
                    base_url = list(self.mentioned_urls)[0]  # Use first mentioned URL
                    navigation_plan = self.browser_navigator.create_navigation_plan(
                        base_url, task
                    )
                    if navigation_plan:
                        logger.info(
                            f"ðŸŒ Created website navigation plan ({len(navigation_plan)} steps): {navigation_plan}"
                        )
                    else:
                        logger.info("ðŸŒ No specific navigation plan created.")
            except Exception as e:
                logger.error(
                    f"Error during task analysis or planning: {e}", exc_info=True
                )
        else:
            logger.warning("TaskAnalyzer not available.")

    async def execute_tool(self, command: ToolCall) -> str:
        """Override execute_tool to add memory tracking, suggestions, and robust error handling."""
        if not command or not command.function or not command.function.name:
            return "Error: Invalid command format provided."

        name = command.function.name
        args_str = command.function.arguments or "{}"
        args = {}
        observation = ""

        try:
            args = json.loads(args_str)
        except json.JSONDecodeError:
            error_msg = f"Invalid JSON arguments for tool {name}: {args_str}"
            logger.error(error_msg)
            self.tool_tracker.record_tool_usage(
                name, {"arguments": args_str}, "failure", "Invalid JSON arguments"
            )
            return f"Error: {error_msg}"

        if name not in self.available_tools.tool_map:
            error_msg = f"Unknown tool '{name}' called."
            logger.error(error_msg)
            self.tool_tracker.record_tool_usage(name, args, "failure", "Unknown tool")
            return f"Error: {error_msg}"

        logger.info(f"Executing Tool: {name} with args: {args}")

        # --- Special Handling Logic ---
        if name == "str_replace_editor":
            return await self._handle_str_replace_editor(
                command
            )  # Uses updated command if modified
        if name == "python_execute":
            return await self._handle_python_execute(
                command
            )  # Uses updated command if modified
        if name == "browser_use":
            action = args.get("action") or args.get(
                "url"
            )  # Get action or URL if action is missing
            if not action:
                return "Error: browser_use requires an 'action' or 'url'."
            # Use dedicated handler which returns formatted observation
            return await self.execute_browser_use(action, **args)

        # --- Suggestion Logic for ask_human ---
        if name == "ask_human":
            question = args.get("inquire")
            if question:
                if self._is_redundant_question(question):
                    observation = (
                        f"Observation: Question '{question}' seems redundant. Skipping."
                    )
                    logger.warning(f"Redundant question skipped: {question}")
                    # Return observation directly, no tool execution needed
                    return observation

                better_tool_suggestion = await self._suggest_better_tool(name, args)
                if better_tool_suggestion:
                    suggested_tool_name = better_tool_suggestion.get("tool")
                    suggested_args = better_tool_suggestion.get("args", {})
                    logger.info(
                        f"ðŸ”„ Intercepted '{name}'. Using better tool: '{suggested_tool_name}' for question: '{question}'"
                    )

                    # Create a new ToolCall for the suggested tool
                    suggested_command = ToolCall(
                        id=f"suggested_{command.id}",
                        type="function",
                        function={
                            "name": suggested_tool_name,
                            "arguments": json.dumps(suggested_args),
                        },
                    )
                    # Execute the suggested tool using this same execute_tool flow (recursive but safe due to checks)
                    return await self.execute_tool(suggested_command)
            # If no suggestion or not a question, proceed with SmartAskHuman via super().execute_tool

        # --- Standard Tool Execution via Base Class ---
        try:
            # Let ToolCallAgent handle the actual execution and basic observation formatting
            observation = await super().execute_tool(command)
            # Record success (assuming super().execute_tool raises exception on failure)
            self.tool_tracker.record_tool_usage(name, args, "success")
            logger.info(f"Tool '{name}' executed successfully.")
            logger.debug(f"Tool '{name}' observation: {observation[:500]}...")

        except Exception as e:
            error_msg = f"Error executing tool '{name}' with args {args_str}: {str(e)}"
            logger.error(error_msg, exc_info=True)
            self.tool_tracker.record_tool_usage(name, args, "failure", str(e))
            # Return a formatted error observation for the LLM
            observation = f"Error: Tool `{name}` failed with error: {str(e)}"

        return observation  # Return formatted observation (success or error)

    # List of realistic user agents for rotation
    REALISTIC_USER_AGENTS = [
        # Chrome on Windows
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36",
        # Chrome on macOS
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36",
        # Firefox on Windows
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/113.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/112.0",
        # Firefox on macOS
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/113.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/112.0",
        # Safari on macOS
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Safari/605.1.15",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.3 Safari/605.1.15",
        # Edge on Windows
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36 Edg/113.0.1774.42",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36 Edg/112.0.1722.58",
    ]
    
    def _get_random_user_agent(self):
        """Return a random user agent from the list of realistic user agents."""
        import random
        return random.choice(self.REALISTIC_USER_AGENTS)
    
    async def _apply_stealth_techniques(self, **kwargs):
        """Apply various stealth techniques to avoid bot detection."""
        url = kwargs.get("url", "")
        browser_tool = self.available_tools.get_tool("browser_use")
        if not browser_tool:
            return
            
        # Apply stealth techniques using JavaScript
        stealth_script = """
        () => {
            // Override navigator properties to make detection harder
            const newProto = navigator.__proto__;
            delete newProto.webdriver;
            
            // Override permissions
            const originalQuery = window.navigator.permissions.query;
            window.navigator.permissions.query = (parameters) => (
                parameters.name === 'notifications' ?
                Promise.resolve({ state: Notification.permission }) :
                originalQuery(parameters)
            );
            
            // Prevent automation detection
            Object.defineProperty(navigator, 'plugins', {
                get: () => [
                    {
                        0: {type: "application/pdf", suffixes: "pdf", description: "Portable Document Format"},
                        description: "PDF Viewer",
                        filename: "internal-pdf-viewer",
                        length: 1,
                        name: "PDF Viewer"
                    },
                    {
                        0: {type: "application/pdf", suffixes: "pdf", description: "Portable Document Format"},
                        description: "Chrome PDF Viewer",
                        filename: "internal-pdf-viewer",
                        length: 1,
                        name: "Chrome PDF Viewer"
                    },
                    {
                        0: {type: "application/x-google-chrome-pdf", suffixes: "pdf", description: "Portable Document Format"},
                        description: "Chrome PDF Plugin",
                        filename: "internal-pdf-viewer",
                        length: 1,
                        name: "Chrome PDF Plugin"
                    }
                ]
            });
            
            // Add language and platform details
            Object.defineProperty(navigator, 'languages', {
                get: () => ['en-US', 'en', 'es', 'fr']
            });
            
            return true;
        }
        """
        
        try:
            # Execute the stealth script
            await browser_tool.execute("evaluate", script=stealth_script)
            
            # Add random delays to simulate human behavior
            if url and ("google.com" in url or "duckduckgo.com" in url):
                import time
                import random
                
                # Random delay between 1-3 seconds
                delay = random.uniform(1, 3)
                time.sleep(delay)
                
                # For search engines, add additional human-like behavior
                human_behavior_script = """
                () => {
                    // Simulate mouse movements
                    function simulateMouseMovement() {
                        const events = ['mouseover', 'mousedown', 'mouseup', 'click'];
                        const elements = document.querySelectorAll('a, button, input');
                        if (elements.length > 0) {
                            const randomElement = elements[Math.floor(Math.random() * elements.length)];
                            events.forEach(event => {
                                randomElement.dispatchEvent(new MouseEvent(event, {
                                    bubbles: true,
                                    cancelable: true,
                                    view: window
                                }));
                            });
                        }
                        return true;
                    }
                    
                    // Simulate scrolling
                    function simulateScrolling() {
                        const scrollAmount = Math.floor(Math.random() * 500) + 100;
                        window.scrollBy(0, scrollAmount);
                        setTimeout(() => {
                            window.scrollBy(0, Math.floor(Math.random() * 200) + 100);
                        }, Math.random() * 1000 + 500);
                        return true;
                    }
                    
                    simulateMouseMovement();
                    return simulateScrolling();
                }
                """
                await browser_tool.execute("evaluate", script=human_behavior_script)
                
                # Add another small random delay
                time.sleep(random.uniform(0.5, 1.5))
                
        except Exception as e:
            logger.warning(f"Failed to apply stealth techniques: {str(e)}")
    
    async def execute_browser_use(self, action_or_url: str, **kwargs):
        """Execute browser use tool with enhanced navigation, overlay handling, and context storage."""
        action = action_or_url
        url = kwargs.get("url", "")
        final_content = ""  # Initialize content string

        try:
            self.browser_used = True

            # Normalize action/url if called simply with a URL
            if action.startswith("http"):
                url = action
                action = "go_to_url"
                kwargs["url"] = url
            elif (
                not url
                and action != "get_html"
                and "selector" not in kwargs
                and "script" not in kwargs
                and "key" not in kwargs
            ):
                raise ValueError(
                    f"Invalid browser_use call: action='{action}' needs url, selector, script, or key."
                )
                
            # Set a random user agent for search engine requests
            if url and ("google.com" in url or "duckduckgo.com" in url or "bing.com" in url):
                # Apply user agent rotation and stealth techniques
                random_user_agent = self._get_random_user_agent()
                if "headers" not in kwargs:
                    kwargs["headers"] = {}
                kwargs["headers"]["User-Agent"] = random_user_agent
                
                # Log the user agent being used
                logger.info(f"Using User-Agent: {random_user_agent}")
                
                # Apply stealth techniques before navigation
                if action == "go_to_url":
                    await self._apply_stealth_techniques(**kwargs)

            logger.info(
                f"Executing browser action: '{action}' | URL: '{url}' | Other args: { {k:v for k,v in kwargs.items() if k != 'url' and k != 'headers'} }"
            )

            browser_tool = self.available_tools.get_tool("browser_use")
            if not browser_tool:
                raise RuntimeError("BrowserUseTool not found.")

            # --- Core Browser Action Execution ---
            raw_result = await browser_tool.execute(action, **kwargs)
            raw_content_str = str(raw_result) if raw_result is not None else ""
            logger.debug(
                f"Browser action '{action}' raw result type: {type(raw_result)}, length: {len(raw_content_str)}"
            )

            # --- Post-Action Processing (Navigation, Content Extraction, etc.) ---
            if action == "go_to_url" and url:
                self.visited_urls.add(url)
                logger.info(
                    f"Navigation to {url} successful (initial). Handling overlays and extracting content..."
                )

                # Handle Overlays AFTER Navigation attempt
                await self._handle_website_overlays()

                # Get Content AFTER Overlays Handled
                extracted_content = ""
                try:
                    # Scroll first to trigger lazy loading
                    scroll_script = "window.scrollTo(0, document.body.scrollHeight / 4); setTimeout(() => window.scrollTo(0, 0), 250);"
                    await browser_tool.execute("evaluate", script=scroll_script)
                    time.sleep(0.5)

                    # Try extracting main content via JS (more lightweight)
                    js_extract_script = """
                     function extractMainText() {
                         const selectors = ['main', 'article', '.content', '#content', '.main', '#main', '[role="main"]', '.post-content', '.entry-content'];
                         for (const selector of selectors) {
                             const element = document.querySelector(selector);
                             if (element && element.innerText.trim().length > 200) return element.innerText;
                         }
                         return document.body.innerText; // Fallback
                     }
                     extractMainText();
                     """
                    js_content = await browser_tool.execute(
                        "evaluate", script=js_extract_script
                    )
                    if js_content and len(str(js_content)) > 100:
                        extracted_content = str(js_content).strip()
                        logger.info(
                            f"Extracted main text via JS for {url}, length: {len(extracted_content)}"
                        )
                    else:
                        logger.info(
                            "JS text extraction failed or too short, getting full HTML."
                        )
                        html = await browser_tool.execute(
                            "evaluate", script="document.documentElement.outerHTML"
                        )
                        if html and not str(html).startswith("Error:"):
                            extracted_content = self._extract_main_content(str(html))
                            logger.info(
                                f"Extracted main content via HTML parsing for {url}, length: {len(extracted_content)}"
                            )
                        else:
                            logger.warning(
                                f"Failed to get HTML for {url}. Using raw result if available."
                            )
                            extracted_content = (
                                raw_content_str if len(raw_content_str) > 50 else ""
                            )

                except Exception as e:
                    logger.error(
                        f"Error extracting content after navigation to {url}: {e}",
                        exc_info=True,
                    )
                    extracted_content = f"Error extracting content from {url}: {e}"  # Report error in content

                # Store & Contextualize meaningful content
                if (
                    extracted_content
                    and not extracted_content.startswith("Error:")
                    and len(extracted_content) > 50
                ):
                    memory_text = (
                        f"Content from {url}:\n{extracted_content[:self.max_observe]}"
                        + ("..." if len(extracted_content) > self.max_observe else "")
                    )
                    self.add_to_context(
                        memory_text,
                        source="browser",
                        tags=["web_content", "visited_url", url],
                    )
                    # Process for task relevance
                    self._process_content_for_task(url, extracted_content)
                    final_content = (
                        extracted_content  # Use extracted content for observation
                    )

                else:  # Handle cases where extraction failed or yielded little
                    logger.warning(f"No meaningful content extracted from {url}.")
                    fail_msg = (
                        f"Visited {url} but could not extract significant content."
                    )
                    self.add_to_context(
                        fail_msg,
                        source="browser",
                        tags=["web_content", "visited_url", url, "extraction_failed"],
                    )
                    final_content = fail_msg  # Use this message for observation

            else:
                # For other actions (click, evaluate, get_html), use the raw result
                final_content = raw_content_str
                # Optionally store results of evaluations etc. in memory if needed
                if (
                    action == "evaluate" and final_content and len(final_content) < 2000
                ):  # Store non-huge eval results
                    self.add_to_context(
                        f"Result of evaluate script: {final_content[:300]}...",
                        source="browser_evaluate",
                        priority="low",
                        tags=["evaluate_result"],
                    )

            # --- Prepare Observation ---
            observation_content = final_content[: self.max_observe] + (
                "..." if len(final_content) > self.max_observe else ""
            )
            observation = f"Observed output of cmd `browser_use` ({action}) executed:\n{observation_content}"
            return observation

        except Exception as e:
            error_msg = (
                f"Error executing browser action '{action}' with {kwargs}: {str(e)}"
            )
            logger.error(error_msg, exc_info=True)
            self.add_to_context(
                f"Failed browser action: {action}. Error: {str(e)}",
                source="browser_error",
                priority="high",
                tags=["error", "browser"],
            )
            return f"Error: Browser action `{action}` failed: {str(e)}"

    async def perform_web_research(self, query: str, max_sites: int = 10) -> str:
        """Perform web research: Search (DDG HTML) -> Extract URLs -> Visit -> Extract Content -> Analyze."""
        urls_to_visit = []
        search_engine_url = ""
        logger.info(f"ðŸ”¬ Starting web research for query: '{query}'")
        search_page_html = None  # Initialize

        # --- Step 1: Search using DuckDuckGo HTML ---
        try:
            search_query_encoded = requests.utils.quote(query)
            search_engine_url = f"https://html.duckduckgo.com/html/?q={search_query_encoded}"
            logger.info(f"Searching DuckDuckGo HTML: {search_engine_url}")
        except Exception as e:
            logger.error(f"Error encoding search query: {str(e)}")
            return f"Error: Failed to encode search query. {str(e)}"
            
        try:

            # --- Use execute_browser_use wrapper ---
            # Navigate first
            navigation_observation = await self.execute_browser_use('go_to_url', url=search_engine_url)

            # Check if navigation itself failed
            if isinstance(navigation_observation, str) and navigation_observation.startswith("Error:"):
                logger.error(f"Initial navigation failed: {navigation_observation}")
                # Attempt Google fallback maybe? Or just return error.
                # For now, return error:
                return f"Error: Could not navigate to search engine. {navigation_observation}"

            # Handle potential overlays on the search results page itself
            await self._handle_website_overlays()

            # Now get the HTML using the wrapper *after* overlays handled
            logger.info("Attempting to retrieve HTML content after overlay handling...")
            html_observation = await self.execute_browser_use('evaluate', script="document.documentElement.outerHTML")

            # --- Extract actual HTML from the observation ---
            if isinstance(html_observation, str):
                if html_observation.startswith("Observed output"):
                    search_page_html = html_observation.split("executed:\n", 1)[-1]
                    logger.info(f"Successfully retrieved search page HTML (length: {len(search_page_html)}).")
                elif html_observation.startswith("Error:"):
                    # THIS is where the error message from the log likely ended up
                    logger.error(f"Failed to retrieve HTML content via evaluate: {html_observation}")
                    search_page_html = None  # Explicitly mark as failed
                else:
                    # Handle unexpected observation format
                    logger.warning(f"Unexpected observation format when getting HTML: {html_observation[:200]}...")
                    search_page_html = html_observation  # Try to use it anyway
            else:
                # If it's not a string, it might be the raw HTML or some other object
                search_page_html = str(html_observation)
                logger.info(f"Retrieved non-string HTML content (type: {type(html_observation)}, length: {len(search_page_html)})")
            # --- End using wrapper ---

            # --- Check the retrieved HTML ---
            # Now the check makes more sense: check if search_page_html is None or too short
            if search_page_html is None or len(search_page_html) < 1500:
                # The log message below should now reflect the actual failure reason from html_observation if it was an error
                log_reason = html_observation if html_observation and isinstance(html_observation, str) and html_observation.startswith("Error:") else f"HTML too short ({len(str(search_page_html) if search_page_html else 'None')}) chars or None."
                logger.warning(f"Failed to retrieve valid HTML from DDG search: {log_reason}")
                
                # Try Google as a fallback
                logger.info("Trying Google as a fallback search engine")
                try:
                    google_search_url = f"https://www.google.com/search?q={search_query_encoded}"
                    logger.info(f"Searching Google: {google_search_url}")
                    
                    # Navigate to Google search
                    await self.execute_browser_use("go_to_url", url=google_search_url)
                    
                    # Handle potential overlays
                    await self._handle_website_overlays()
                    
                    # Wait a bit for dynamic content to load
                    import time
                    time.sleep(2)
                    
                    # Extract URLs using our specialized JavaScript function
                    js_extract_script = """
                    function extractSearchResults() {
                        const results = [];
                        const seenUrls = new Set();
                        
                        // Try various selectors for Google search results
                        const searchSelectors = [
                            '.g .yuRUbf > a',                  // Modern Google search result links
                            '.g div > a[href^="http"]',       // Alternative structure
                            '.rc .r > a',                       // Older Google structure
                            '[data-header-feature] a',          // Featured snippets
                            '.g a[ping]',                       // Links with ping attribute
                            'div[data-hveid] a[href^="http"]', // General result containers
                            'div[data-sokoban-feature] a[href^="http"]', // Another container type
                            'a[jsname]',                        // Google's jsname links
                            'a[data-ved]'                       // Links with data-ved attribute
                        ];
                        
                        // Try each selector
                        for (const selector of searchSelectors) {
                            try {
                                const links = document.querySelectorAll(selector);
                                if (links.length > 0) {
                                    console.log(`Found ${links.length} links with selector: ${selector}`);
                                }
                                
                                for (const link of links) {
                                    try {
                                        const url = link.href;
                                        
                                        // Skip if we've already seen this URL or if it's a Google internal URL
                                        if (seenUrls.has(url) || 
                                            url.includes('google.com/search') || 
                                            url.includes('google.com/imgres') || 
                                            url.includes('accounts.google') ||
                                            url.includes('/amp/') ||
                                            url.includes('webcache.googleusercontent') ||
                                            url.includes('google.com/url?') ||
                                            url.endsWith('.jpg') || 
                                            url.endsWith('.jpeg') || 
                                            url.endsWith('.png') || 
                                            url.endsWith('.gif') || 
                                            url.endsWith('.css') || 
                                            url.endsWith('.js')) {
                                            continue;
                                        }
                                        
                                        // Add to results and mark as seen
                                        results.push({
                                            url: url,
                                            title: link.textContent.trim() || 'Untitled'
                                        });
                                        seenUrls.add(url);
                                    } catch (linkError) {
                                        console.log('Error processing link:', linkError);
                                    }
                                }
                            } catch (selectorError) {
                                console.log('Error with selector:', selector, selectorError);
                            }
                        }
                        
                        // If we still don't have results, try a more aggressive approach
                        if (results.length === 0) {
                            console.log('No results found with standard selectors, trying aggressive approach');
                            // Get all links on the page
                            try {
                                const allLinks = document.querySelectorAll('a[href^="http"]');
                                
                                for (const link of allLinks) {
                                    try {
                                        const url = link.href;
                                        
                                        // Skip if we've already seen this URL or if it's a Google internal URL
                                        if (seenUrls.has(url) || 
                                            url.includes('google.com/search') || 
                                            url.includes('google.com/imgres') || 
                                            url.includes('accounts.google') ||
                                            url.includes('/amp/') ||
                                            url.includes('webcache.googleusercontent') ||
                                            url.includes('google.com/url?') ||
                                            url.endsWith('.jpg') || 
                                            url.endsWith('.jpeg') || 
                                            url.endsWith('.png') || 
                                            url.endsWith('.gif') || 
                                            url.endsWith('.css') || 
                                            url.endsWith('.js')) {
                                            continue;
                                        }
                                        
                                        results.push({
                                            url: url,
                                            title: link.textContent.trim() || 'Untitled'
                                        });
                                        seenUrls.add(url);
                                    } catch (linkError) {
                                        console.log('Error processing link in aggressive approach:', linkError);
                                    }
                                }
                            } catch (allLinksError) {
                                console.log('Error getting all links:', allLinksError);
                            }
                        }
                        
                        console.log(`Total unique URLs found: ${results.length}`);
                        return results;
                    }
                    return extractSearchResults();
                    """
                    
                    search_results = await self.execute_browser_use("evaluate", script=js_extract_search_results)
                    
                    if search_results and isinstance(search_results, list) and len(search_results) > 0:
                        # Extract just the URLs from the results
                        google_urls = [result['url'] for result in search_results if 'url' in result]
                        logger.info(f"Extracted {len(google_urls)} URLs from Google search")
                        
                        if google_urls:
                            urls_to_visit = google_urls[:max_sites]
                            logger.info("Successfully extracted URLs from Google fallback")
                        else:
                            # If we couldn't extract URLs from Google either, try Bing as a last resort
                            logger.warning("Could not extract URLs from Google search, trying Bing as a last resort")
                            bing_urls = await self._fallback_bing_search(query)
                            
                            if bing_urls:
                                urls_to_visit = bing_urls[:max_sites]
                                logger.info(f"Successfully extracted {len(urls_to_visit)} URLs from Bing fallback search")
                            else:
                                # If all search engines fail, return an error
                                logger.error("All search engines failed to provide usable URLs")
                                return "Error: Could not extract any URLs from multiple search engines. Please try a different query or try again later."
                    else:
                        # If Google fails too, try Bing as a last resort
                        logger.warning("Failed to extract URLs from Google search, trying Bing as a last resort")
                        bing_urls = await self._fallback_bing_search(query)
                        
                        if bing_urls:
                            urls_to_visit = bing_urls[:max_sites]
                            logger.info(f"Successfully extracted {len(urls_to_visit)} URLs from Bing fallback search")
                        else:
                            # If all search engines fail, return an error
                            logger.error("All search engines failed to provide usable URLs")
                            return "Error: Could not extract any URLs from multiple search engines. Please try a different query or try again later."
                except Exception as e:
                    logger.error(f"Error during Google fallback search: {str(e)}")
                    # Try Bing as a last resort
                    logger.info("Google fallback failed, trying Bing as a last resort")
                    try:
                        bing_urls = await self._fallback_bing_search(query)
                        
                        if bing_urls:
                            urls_to_visit = bing_urls[:max_sites]
                            logger.info(f"Successfully extracted {len(urls_to_visit)} URLs from Bing fallback search")
                        else:
                            # If all search engines fail, return an error
                            logger.error("All search engines failed to provide usable URLs")
                            return "Error: Could not extract any URLs from multiple search engines. Please try a different query or try again later."
                    except Exception as bing_error:
                        logger.error(f"Error during Bing fallback search: {str(bing_error)}")
                        return f"Error: All search engines failed. {str(e)} and {str(bing_error)}"
                    
                    # End of exception handling block
                    
                    # Extract URLs using our specialized JavaScript function
                    js_extract_search_results = """
                    function extractSearchResults() {
                        const results = [];
                        const seenUrls = new Set();
                        
                        // Try various selectors for Google search results
                        const searchSelectors = [
                            '.g .yuRUbf > a',                  // Modern Google search result links
                            '.g div > a[href^="http"]',       // Alternative structure
                            '.rc .r > a',                       // Older Google structure
                            '[data-header-feature] a',          // Featured snippets
                            '.g a[ping]',                       // Links with ping attribute
                            'div[data-hveid] a[href^="http"]', // General result containers
                            'div[data-sokoban-feature] a[href^="http"]', // Another container type
                            'a[jsname]',                        // Google's jsname links
                            'a[data-ved]'                       // Links with data-ved attribute
                        ];
                        
                        // Try each selector
                        for (const selector of searchSelectors) {
                            try {
                                const links = document.querySelectorAll(selector);
                                if (links.length > 0) {
                                    console.log(`Found ${links.length} links with selector: ${selector}`);
                                }
                                
                                for (const link of links) {
                                    try {
                                        const url = link.href;
                                        
                                        // Skip if we've already seen this URL or if it's a Google internal URL
                                        if (seenUrls.has(url) || 
                                            url.includes('google.com/search') || 
                                            url.includes('google.com/imgres') || 
                                            url.includes('accounts.google') ||
                                            url.includes('/amp/') ||
                                            url.includes('webcache.googleusercontent') ||
            else:
                logger.info(f"Successfully retrieved search page HTML (length: {len(str(search_page_html))}).")
                logger.info("Attempting to extract URLs from DuckDuckGo HTML results...")
                urls_to_visit = self._extract_ddg_search_results(str(search_page_html))

            if urls_to_visit:
                logger.info(f"Extracted {len(urls_to_visit)} potential URLs from DDG.")
                urls_to_visit = [
                    u
                    for u in urls_to_visit
                    if not u.lower().endswith((".pdf", ".xml", ".zip"))
                ][:max_sites]
                logger.info(f"Selected {len(urls_to_visit)} URLs to visit: {urls_to_visit}")
            else:
                logger.warning(f"No valid URLs extracted from DDG results for: '{query}'")
                # Return more informative message
                return f"Web research for '{query}' returned search results, but no valid URLs could be extracted from them."

        except Exception as e:
            logger.error(
                f"Error during search engine interaction or URL extraction: {e}",
                exc_info=True,
            )
            return f"Error: Failed during web search phase for '{query}'. {str(e)}"

        # --- Step 2: Visit URLs and Extract Content ---
        processed_content = []  # Store {'url': url, 'content': text}
        visited_count = 0
        if not urls_to_visit:
            # This case should be handled above, but added for safety
            return "No search result URLs found to visit."

        for i, url in enumerate(urls_to_visit):
            if visited_count >= max_sites:
                break

            logger.info(f"Visiting URL {i+1}/{len(urls_to_visit)}: {url}")
            try:
                # execute_browser_use handles navigation, overlays, extraction
                # It returns the observation string which includes the content or error
                observation = await self.execute_browser_use("go_to_url", url=url)

                # Extract content from the observation string if successful
                if isinstance(observation, str):
                    if observation.startswith("Observed output"):
                        # Extract content after the preamble
                        content_part = observation.split("executed:\n", 1)[-1]
                        # Check if content seems valid (not an error message disguised as content)
                        if content_part and len(content_part) > 50 and not (content_part.startswith("Visited") or "extract significant content" in content_part):
                            logger.info(f"Successfully retrieved content for {url}, length: {len(content_part)}")
                            processed_content.append({"url": url, "content": content_part})
                            visited_count += 1
                        else:
                            logger.warning(f"Skipping URL {url} due to apparently missing/failed content extraction: {content_part[:100]}...")
                    elif observation.startswith("Error:"):
                        logger.warning(f"Skipping URL {url} due to error during visit/extraction: {observation[:200]}")
                    else:
                        # Try to use the observation as content anyway
                        if len(observation) > 100:
                            logger.info(f"Using unexpected observation format as content for {url}, length: {len(observation)}")
                            processed_content.append({"url": url, "content": observation})
                            visited_count += 1
                        else:
                            logger.warning(f"Skipping URL {url} due to short/invalid content: {observation[:100]}")
                else:
                    # If it's not a string but some other object, try to use it as content
                    content_str = str(observation)
                    if len(content_str) > 100:
                        logger.info(f"Using non-string observation as content for {url}, length: {len(content_str)}")
                        processed_content.append({"url": url, "content": content_str})
                        visited_count += 1
                    else:
                        logger.warning(f"Skipping URL {url} due to invalid non-string content type: {type(observation)}")

                if i < len(urls_to_visit) - 1:
                    time.sleep(1.5)  # Delay

            except Exception as e:
                logger.error(f"Critical error processing URL {url}: {e}", exc_info=True)

        # --- Step 3: Analyze Collected Content ---
        logger.info(f"Finished visiting URLs. Collected content from {visited_count} sites.")

        if not processed_content:
            logger.warning(f"No content collected for query: '{query}'")
            return "Web research completed, but no content was successfully extracted from the result URLs."

        combined_content_for_analysis = ""
        sources_metadata = []
        for idx, item in enumerate(processed_content):
            combined_content_for_analysis += f"\n\n--- SOURCE {idx+1}: {item['url']} ---\n\n{item['content']}"
            sources_metadata.append({"id": idx + 1, "url": item["url"]})

        logger.info(f"Combined content length for analysis: {len(combined_content_for_analysis)}")

        analysis_result = await self._analyze_combined_content(
            combined_content_for_analysis, query, sources_metadata
        )
        return f"Web research result for query '{query}':\n{analysis_result}"  # Add context

    async def _fallback_bing_search(self, query: str) -> List[str]:
        """Use Bing as a fallback search engine when other methods fail."""
        try:
            logger.info(f"Using Bing fallback search for query: '{query}'")
            search_query_encoded = requests.utils.quote(query)
            bing_url = f"https://www.bing.com/search?q={search_query_encoded}"
            
            # Use a different user agent for Bing
            random_user_agent = self._get_random_user_agent()
            
            # Navigate to Bing with our stealth techniques
            await self.execute_browser_use("go_to_url", url=bing_url)
            
            # Add a delay to let the page load
            import time
            time.sleep(2)
            
            # Extract URLs using a simpler script for Bing to avoid syntax issues
            bing_extract_script = '''
            function extractBingResults() {
                const results = [];
                const seenUrls = new Set();
                
                // Get all links on the page
                const allLinks = document.querySelectorAll('a[href^="http"]');
                
                for (const link of allLinks) {
                    try {
                        const url = link.href;
                        if (!url) continue;
                        
                        // Skip if already seen or if it's a Bing URL
                        if (seenUrls.has(url) || 
                            url.includes("bing.com") || 
                            url.includes("microsoft.com") || 
                            url.includes("msn.com") || 
                            url.endsWith(".jpg") || 
                            url.endsWith(".jpeg") || 
                            url.endsWith(".png") || 
                            url.endsWith(".gif") || 
                            url.endsWith(".pdf") || 
                            url.endsWith(".css") || 
                            url.endsWith(".js")) {
                            continue;
                        }
                        
                        // Get the title
                        const title = link.textContent.trim() || "Untitled";
                        
                        results.push({
                            url: url,
                            title: title
                        });
                        seenUrls.add(url);
                    } catch (e) {
                        console.log("Error processing link:", e);
                    }
                }
                
                console.log("Extracted " + results.length + " URLs from Bing");
                return results;
            }
            return extractBingResults();
            '''
            
            search_results = await self.execute_browser_use("evaluate", script=bing_extract_script)
            
            if search_results and isinstance(search_results, list) and len(search_results) > 0:
                # Extract just the URLs from the results
                bing_urls = [result['url'] for result in search_results if 'url' in result]
                logger.info(f"Successfully extracted {len(bing_urls)} URLs from Bing fallback search")
                return bing_urls
            else:
                logger.warning("Failed to extract URLs from Bing fallback search")
                return []
                
        except Exception as e:
            logger.error(f"Error during Bing fallback search: {str(e)}")
            return []
    
    def _extract_google_search_results(self, html: str) -> List[str]:
        """Extracts organic search result URLs from Google HTML content."""
        urls = set()
        try:
            soup = BeautifulSoup(html, "html.parser")
            for link in soup.find_all("a", href=True):
                href = link["href"]
                if href.startswith("/url?q="):
                    try:
                        parsed_url = urlparse(href)
                        query_params = parse_qs(parsed_url.query)
                        if "q" in query_params:
                            actual_url = query_params["q"][0]
                            if (
                                actual_url.startswith("http")
                                and "google.com" not in urlparse(actual_url).netloc
                            ):
                                urls.add(unquote(actual_url))
                    except Exception as e:
                        logger.debug(f"Error parsing Google redirect {href}: {e}")
                elif (
                    href.startswith("http")
                    and "google.com" not in urlparse(href).netloc
                ):
                    # Check parent structure for direct links (less reliable)
                    parent_h3 = link.find_parent("h3")
                    if parent_h3:
                        result_block = parent_h3.find_parent(
                            ["div", "li"], class_=re.compile(r"\bg\b|rc|kvH3mc|VtXmk")
                        )
                        if result_block:
                            urls.add(unquote(href))

            logger.info(f"Extracted {len(urls)} unique URLs from Google HTML.")
            return list(urls)
        except Exception as e:
            logger.error(
                f"Error parsing Google search results HTML: {e}", exc_info=True
            )
            return []

    def _extract_ddg_search_results(self, html: str) -> List[str]:
        """Extracts organic search result URLs from DuckDuckGo HTML version."""
        urls = set()
        try:
            soup = BeautifulSoup(html, "html.parser")
            # Main results are in divs with class="result" containing h2 with class="result__title" and a with class="result__a"
            for result_div in soup.select("div.result"):
                link_tag = result_div.select_one("a.result__a")
                if link_tag and link_tag.has_attr("href"):
                    href = link_tag["href"]
                    try:
                        if href.startswith("/l/?kh="):  # DDG redirect
                            parsed_href = urlparse(href)
                            query_params = parse_qs(parsed_href.query)
                            if "uddg" in query_params:
                                actual_url = query_params["uddg"][0]
                                if actual_url.startswith("http"):
                                    urls.add(unquote(actual_url))
                        elif (
                            href.startswith("http")
                            and "duckduckgo.com" not in urlparse(href).netloc
                        ):
                            urls.add(
                                unquote(href)
                            )  # Direct link (less common in HTML version)
                    except Exception as e:
                        logger.debug(f"Error parsing DDG link {href}: {e}")

            logger.info(f"Extracted {len(urls)} unique URLs from DuckDuckGo HTML.")
            return list(urls)
        except Exception as e:
            logger.error(
                f"Error parsing DuckDuckGo search results HTML: {e}", exc_info=True
            )
            return []

    def _extract_main_content(self, html: str) -> str:
        """Extract the main textual content from an HTML page using multiple strategies."""
        if not html:
            return ""
        try:
            try:
                soup = BeautifulSoup(html, "lxml")  # Prefer lxml for speed/robustness
            except ImportError:
                try:
                    soup = BeautifulSoup(html, "html.parser")
                except Exception as parse_err:
                    logger.error(
                        f"HTML parsing failed: {parse_err}. Falling back to regex."
                    )
                    text = re.sub(
                        r"<script.*?</script>|<style.*?</style>",
                        "",
                        html,
                        flags=re.DOTALL | re.IGNORECASE,
                    )
                    text = re.sub(r"<[^>]+>", " ", text)
                    text = re.sub(r"\s+", " ", text).strip()
                    return text[: self.max_observe]

            for element in soup(
                [
                    "script",
                    "style",
                    "nav",
                    "header",
                    "footer",
                    "aside",
                    "form",
                    "button",
                    "iframe",
                    "noscript",
                    "figure",
                    "img",
                    "svg",
                    "path",
                    "meta",
                    "link",
                ]
            ):
                element.decompose()

            main_content_element = None
            selectors = [
                "article",
                "main",
                ".main-content",
                "#main-content",
                ".post-content",
                ".entry-content",
                ".page-content",
                ".content",
                "#content",
                ".article-body",
                "#bodyContent",
                ".story-content",
                ".article",
            ]
            for selector in selectors:
                try:
                    element = soup.select_one(selector)
                    if element and len(element.get_text(strip=True)) > 300:
                        main_content_element = element
                        logger.info(f"Found main content using selector: '{selector}'")
                        break
                except Exception as e:
                    logger.debug(f"Selector '{selector}' failed: {e}")

            if not main_content_element:  # Strategy 2: Largest text block
                largest_container = None
                max_len = 0
                for tag in soup.find_all(["div", "section"]):
                    if tag.find(["article", "main"]):
                        continue  # Avoid double counting
                    text = tag.get_text(strip=True)
                    text_len = len(text)
                    if text_len > max_len and text_len > 500:
                        links_text_len = sum(
                            len(a.get_text(strip=True))
                            for a in tag.find_all("a", recursive=False)
                        )  # Less recursive link check
                        if links_text_len < text_len * 0.6:
                            max_len = text_len
                            largest_container = tag
                if largest_container:
                    main_content_element = largest_container
                    logger.info(
                        f"Found main content using largest text container (len: {max_len})"
                    )

            if not main_content_element:  # Strategy 3: Fallback to body
                main_content_element = soup.body
                if main_content_element:
                    logger.info("Using <body> as main content container.")
                else:
                    main_content_element = soup
                    logger.warning("Could not find <body>, using entire soup.")

            if main_content_element:
                text = main_content_element.get_text(separator="\n", strip=True)
                text = re.sub(r"\n\s*\n", "\n\n", text).strip()  # Clean blank lines
                logger.info(f"Extracted text length: {len(text)}")
                return text[: self.max_observe]  # Truncate before returning
            else:
                logger.warning("Failed to find any suitable content container.")
                return ""

        except Exception as e:
            logger.error(f"Error in _extract_main_content: {e}", exc_info=True)
            text = re.sub(
                r"<script.*?</script>|<style.*?</style>",
                "",
                html,
                flags=re.DOTALL | re.IGNORECASE,
            )
            text = re.sub(r"<[^>]+>", " ", text)
            text = re.sub(r"\s+", " ", text).strip()
            return text[: self.max_observe]

    async def _analyze_combined_content(
        self, combined_content: str, query: str, sources: List[Dict]
    ) -> str:
        """Analyze combined content using Ollama API, with saving results."""
        logger.info(
            f"Analyzing combined content (length: {len(combined_content)}) for query: '{query}'"
        )
        if not combined_content:
            return "No content was available for analysis."

        try:
            cfg = get_config()
            ollama_settings = getattr(cfg, "ollama", None)
            if (
                not ollama_settings
                or not ollama_settings.api_base
                or not ollama_settings.model
            ):
                logger.error("Ollama configuration missing. Cannot analyze content.")
                return "Error: Ollama analysis configuration is missing."

            model_name = ollama_settings.model
            temperature = ollama_settings.temperature
            max_tokens = ollama_settings.max_tokens or 4096
            api_base = ollama_settings.api_base.rstrip("/")
            ollama_url = f"{api_base}/api/generate"
            logger.info(
                f"Using Ollama: URL={ollama_url}, Model={model_name}, Temp={temperature}, MaxTokens={max_tokens}"
            )

            max_input_len = 32000  # Safety limit
            if len(combined_content) > max_input_len:
                logger.warning(
                    f"Combined content ({len(combined_content)} chars) exceeds max input guess ({max_input_len}). Truncating."
                )
                combined_content = (
                    combined_content[:max_input_len] + "\n\n[Input Content Truncated]"
                )

            prompt = f"""You are a research analyst. Analyze the following web content collected for the query: "{query}"

--- Collected Content (Sources are marked as '--- SOURCE [Number]: [URL] ---') ---
{combined_content}
--- End Collected Content ---

Based *only* on the text provided above:
1. Answer the query: "{query}"
2. Summarize the key information relevant to the query from all sources.
3. Identify any significant conflicting information or differing perspectives.
4. Highlight the most relevant facts and insights.
5. Cite sources using [Source N] format where N matches the number in the content.

Provide a concise, well-structured response. If the content doesn't answer the query, state that. Do not add external knowledge.
"""

            payload = {
                "model": model_name,
                "prompt": prompt,
                "stream": False,
                "options": {"temperature": temperature, "num_predict": max_tokens},
            }

            logger.info("Sending request to Ollama API for analysis...")
            start_time = time.time()
            response = requests.post(
                ollama_url, json=payload, timeout=180
            )  # Increased timeout
            response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
            end_time = time.time()
            logger.info(
                f"Ollama API request completed in {end_time - start_time:.2f} seconds."
            )

            result = response.json()
            analysis = result.get("response", "").strip()

            if analysis:
                logger.info(
                    f"Successfully received analysis from Ollama API ({len(analysis)} chars)"
                )
                # Save Research Results
                try:
                    results_dir = Path(config.workspace_root) / "research_results"
                    results_dir.mkdir(parents=True, exist_ok=True)
                    query_slug = _slugify(query)[:50]
                    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"research_{query_slug}_{ts}.json"
                    filepath = results_dir / filename
                    research_data = {
                        "query": query,
                        "timestamp": datetime.now().isoformat(),
                        "sources": sources,
                        "analysis": analysis,
                        "raw_combined_content_length": len(combined_content),
                    }
                    with open(filepath, "w", encoding="utf-8") as f:
                        json.dump(research_data, f, indent=2, ensure_ascii=False)
                    logger.info(f"Saved research results to {filepath}")
                except Exception as save_err:
                    logger.error(f"Failed to save research results: {save_err}")
                return analysis
            else:
                logger.error(
                    f"Ollama API returned success but response field is empty. Response: {result}"
                )
                return self._fallback_content_analysis(combined_content, query, sources)

        except requests.exceptions.Timeout:
            logger.error(f"Timeout connecting to Ollama API ({ollama_url})")
            return f"Error: Analysis service timed out. {str(req_err)}"
        except requests.exceptions.RequestException as req_err:
            logger.error(
                f"Network error connecting to Ollama API ({ollama_url}): {req_err}",
                exc_info=True,
            )
            return f"Error: Could not connect to the analysis service. {str(req_err)}"
        except Exception as e:
            logger.error(
                f"Error analyzing combined content with Ollama API: {e}", exc_info=True
            )
            return self._fallback_content_analysis(combined_content, query, sources)

    def _fallback_content_analysis(
        self, combined_content: str, query: str, sources: List[Dict]
    ) -> str:
        """Fallback method if Ollama analysis fails. Provides a structured summary."""
        logger.warning("Executing fallback content analysis.")
        if not combined_content:
            return "Analysis failed: No content was provided."

        output = f"Fallback Analysis for query: '{query}'\n\n"
        output += "Could not perform detailed analysis via LLM. Here's a summary of collected content:\n\n"

        # Split content by source markers more robustly
        source_blocks = re.split(r"--- SOURCE \d+:.*?---", combined_content)
        source_blocks = [
            block.strip() for block in source_blocks if block and block.strip()
        ]  # Clean up empty splits

        if not source_blocks or len(source_blocks) != len(sources):
            logger.warning(
                f"Mismatch/issue splitting content blocks ({len(source_blocks)}) vs sources ({len(sources)}). Summarizing raw content."
            )
            output += f"Raw combined content (first {self.max_observe} chars):\n{combined_content[:self.max_observe]}...\n"
            return output

        for i, source_info in enumerate(sources):
            url = source_info.get("url", f"Unknown Source {i+1}")
            output += f"--- Source {i+1}: {url} ---\n"
            content_snippet = source_blocks[i]
            # Simple summary: first few lines or sentences
            lines = content_snippet.splitlines()
            summary = "\n".join(
                line for line in lines[:5] if line.strip()
            )  # First 5 non-empty lines
            if not summary:  # If very short lines, try sentences
                sentences = re.split(r"(?<=[.!?])\s+", content_snippet)
                summary = " ".join(sentences[:2])  # First 2 sentences
            output += f"Summary Snippet:\n{summary[:500]}...\n\n"

        output += f"\nNote: This is a basic summary due to an analysis error. Full content length: {len(combined_content)}"
        return output

    async def _handle_website_overlays(self):
        """Handle cookie popups, paywalls, and other common website overlays."""
        logger.info("ðŸ›¡ï¸ Attempting to handle website overlays...")
        browser_tool = self.available_tools.get_tool("browser_use")
        if not browser_tool:
            logger.error("BrowserUseTool not available.")
            return

        # Use a list of scripts/actions to try sequentially
        actions_to_try = [
            # 1. Try known JS library handlers
            {
                "type": "evaluate",
                "script": """
            function handleCommonConsentLibs() { /* ... (full script from previous answer) ... */
                let handled = false;
                // OneTrust
                if (typeof OneTrust !== 'undefined') {
                    if (OneTrust.RejectAll) { try { OneTrust.RejectAll(); handled = true; console.log('OneTrust RejectAll executed.'); } catch(e){} }
                    else if (OneTrust.AllowAll) { try { OneTrust.AllowAll(); handled = true; console.log('OneTrust AllowAll executed.'); } catch(e){} }
                }
                // Cookiebot, CookieConsent, Osano, Quantcast, Didomi, Civic etc.
                 if (typeof Cookiebot !== 'undefined' && Cookiebot.submitCustomConsent) { try { Cookiebot.submitCustomConsent(true, true, true); handled = true; console.log('Cookiebot executed.'); } catch(e){} }
                 if (typeof CookieConsent !== 'undefined' && CookieConsent.acceptAll) { try { CookieConsent.acceptAll(); handled = true; console.log('CookieConsent executed.'); } catch(e) {} }
                 if (typeof Osano !== 'undefined' && Osano.cm && Osano.cm.acceptAll) { try { Osano.cm.acceptAll(); handled = true; console.log('Osano executed.'); } catch(e){} }
                 if (typeof __tcfapi !== 'undefined') { try { __tcfapi('acceptAll', 2, () => {}); handled = true; console.log('Quantcast executed.'); } catch(e){} }
                 if (typeof Didomi !== 'undefined' && Didomi.setUserAgreeToAll) { try { Didomi.setUserAgreeToAll(); handled = true; console.log('Didomi executed.'); } catch(e) {} }
                 if (typeof CookieControl !== 'undefined' && CookieControl.acceptAll) { try { CookieControl.acceptAll(); handled = true; console.log('CookieControl executed.'); } catch(e) {} }
                return handled;
             } handleCommonConsentLibs();
            """,
            },
            # 2. Click common buttons
            {
                "type": "evaluate",
                "script": """
            function clickCommonButtons() { /* ... (full script from previous answer) ... */
                const buttons = [];
                const selectors = [ '#onetrust-accept-btn-handler', '.cc-dismiss', '.cc-btn.cc-allow', '.cc-allow', /* ... more selectors ... */ ];
                selectors.forEach(s => { try { buttons.push(...document.querySelectorAll(s)); } catch(e) {} });
                const commonTexts = [ 'Accept', 'Accept All', 'Allow All', 'Agree', /* ... more texts ... */ 'Reject', 'Close', 'Dismiss' ];
                document.querySelectorAll('button, a, [role="button"]').forEach(el => { /* ... add if text matches ... */ });
                 let clicked = false; /* ... (logic to prioritize reject/close, then accept/agree) ... */
                 // Prioritize reject/close/dismiss if found and visible
                 for (const btn of new Set(buttons)) { /* ... (visibility check) ... */ if (['reject', 'deny', 'decline', 'close', 'dismiss'].some(t => btn.textContent.trim().toLowerCase().includes(t))) { btn.click(); clicked = true; break; } }
                 // If no reject/close was clicked, try accept/agree
                 if (!clicked) { for (const btn of new Set(buttons)) { /* ... (visibility check) ... */ if (['accept', 'allow', 'agree', 'ok'].some(t => btn.textContent.trim().toLowerCase().includes(t))) { btn.click(); clicked = true; break; } } }
                return clicked;
             } clickCommonButtons();
            """,
            },
            # 3. Press Escape Key
            {"type": "press", "args": {"selector": "body", "key": "Escape"}},
            # 4. Remove common overlay elements and fix scrolling
            {
                "type": "evaluate",
                "script": """
            function removeOverlaysAndFixScroll() { /* ... (full script from previous answer) ... */
                 let removed = false; const overlaySelectors = [ '.modal', '#modal', /* ... more selectors ... */ ];
                 overlaySelectors.forEach(s => { try { document.querySelectorAll(s).forEach(el => { el.remove(); removed = true; }); } catch(e){} });
                 // Fix body scrolling
                 if (document.body.style.overflow === 'hidden' || document.body.classList.contains('modal-open')) { document.body.style.overflow = 'auto'; document.body.style.position = 'static'; document.body.classList.remove('modal-open', 'no-scroll'); removed = true; }
                 // Fix html scrolling
                 if (document.documentElement.style.overflow === 'hidden') { document.documentElement.style.overflow = 'auto'; removed = true; }
                 return removed;
             } removeOverlaysAndFixScroll();
            """,
            },
        ]

        for i, action_def in enumerate(actions_to_try):
            try:
                action_type = action_def["type"]
                logger.debug(f"Overlay Action {i+1}: Type='{action_type}'")
                if action_type == "evaluate":
                    result = await browser_tool.execute(
                        "evaluate", script=action_def["script"]
                    )
                    logger.debug(f" -> Result: {result}")
                    if result and str(result).lower() == "true":
                        time.sleep(0.7)  # Pause if script reported change
                elif action_type == "press":
                    await browser_tool.execute("press", **action_def["args"])
                    logger.debug(f" -> Executed key press.")
                    time.sleep(0.5)
            except Exception as e:
                logger.debug(f"Error during overlay action {i+1} ({action_type}): {e}")

        logger.info("ðŸ›¡ï¸ Finished overlay handling attempts.")

    def _extract_urls(self, text: str) -> List[str]:
        """Extract all URLs from the given text using a more robust regex."""
        if not text:
            return []
        # Regex to find URLs, avoiding trailing punctuation common in text
        url_pattern = (
            r'https?://[^\s()<>"\'`]+?(?:[\]\)]*[^\s`!()\[\]{};:\'".,<>?Â«Â»â€œâ€â€˜â€™]|$)'
        )
        urls = re.findall(url_pattern, text)
        # Basic validation: ensure it has a domain part
        valid_urls = [u for u in urls if "." in urlparse(u).netloc]
        return valid_urls

    def _should_use_browser(self, user_message: str) -> bool:
        """Determine if browser should be used based on user message content."""
        urls = self._extract_urls(user_message)
        if urls:
            self.mentioned_urls.update(urls)
            logger.info(f"Detected URLs in message: {urls}. Suggesting browser use.")
            return True

        browse_phrases = [
            "check website",
            "visit site",
            "go to page",
            "look at page",
            "on their website",
            "online at",
            "url for",
        ]
        research_keywords = [
            "find info on",
            "research",
            "what is",
            "tell me about",
            "latest news on",
            "details about",
        ]

        msg_lower = user_message.lower()
        if any(phrase in msg_lower for phrase in browse_phrases):
            logger.info("Detected browse phrase. Suggesting browser use.")
            return True

        # If it looks like a research question requiring external info
        if (
            msg_lower.endswith("?")
            and len(user_message) > 15
            and any(kw in msg_lower for kw in research_keywords)
        ):
            logger.info(
                "Detected potential research question. May involve web research."
            )
            # Let LLM decide initially, suggestion logic will catch it later if needed
            return False

        return False

    def _is_redundant_question(self, question: str) -> bool:
        """Check if a question is redundant based on conversation history."""
        if not self._prevent_repeated_questions:
            return False
        question_norm = question.strip().lower()

        # Check using PersistentMemory's similarity check
        try:
            # Assuming is_similar_question is synchronous and takes threshold
            if self.conversation_memory.is_similar_question(
                question_norm, threshold=0.9
            ):
                logger.warning(
                    f"ðŸ”„ Avoiding redundant question (memory similarity): {question}"
                )
                return True
        except Exception as e:
            logger.error(f"Error checking memory for similar question: {e}")

        # Fallback: Check tool tracker history
        if self.tool_tracker.was_question_asked(question_norm):
            logger.warning(
                f"ðŸ”„ Avoiding redundant question (tracker history): {question}"
            )
            return True

        return False

    def _get_most_relevant_url(self, query: str) -> Optional[str]:
        """Get the most relevant URL from mentioned URLs based on query keywords."""
        if not self.mentioned_urls:
            return None
        if len(self.mentioned_urls) == 1:
            return next(iter(self.mentioned_urls))

        query_terms = set(re.findall(r"\w+", query.lower()))
        if not query_terms:
            return list(self.mentioned_urls)[-1]  # Return last if no query terms

        best_url = None
        max_score = -1
        for url in self.mentioned_urls:
            score = 0
            parsed_url = urlparse(url)
            url_text = (
                parsed_url.netloc
                + parsed_url.path
                + parsed_url.query
                + parsed_url.fragment
            ).lower()
            url_terms = set(re.findall(r"\w+", url_text))
            score += len(query_terms.intersection(url_terms))
            # Boost score if term is in domain or last path segment
            if any(term in parsed_url.netloc.lower() for term in query_terms):
                score += 2
            if parsed_url.path:
                last_segment = parsed_url.path.split("/")[-1]
                if any(term in last_segment for term in query_terms):
                    score += 1

            if score > max_score:
                max_score = score
                best_url = url

        return (
            best_url if best_url and max_score >= 0 else list(self.mentioned_urls)[-1]
        )

    async def _handle_str_replace_editor(self, command: ToolCall) -> str:
        """Special handler for str_replace_editor to create files when they don't exist."""
        args = {}
        args_str = command.function.arguments or "{}"
        try:
            args = json.loads(args_str)
            editor_command = args.get("command")
            path_str = args.get("path")

            if not path_str:
                return "Error executing str_replace_editor: Missing 'path' parameter"

            workspace_path = Path(config.workspace_root)
            target_path = (workspace_path / path_str).resolve()

            if not target_path.is_relative_to(workspace_path):
                logger.error(f"Security Alert: Path outside workspace: {target_path}")
                return f"Error: Access denied. Path '{path_str}' is outside workspace."

            logger.info(
                f"Executing str_replace_editor: cmd='{editor_command}', path='{target_path}'"
            )

            if editor_command == "view" and not target_path.exists():
                try:
                    target_path.parent.mkdir(parents=True, exist_ok=True)
                    target_path.touch()
                    logger.info(f"ðŸ“„ Created missing file for viewing: {target_path}")
                    self.tool_tracker.record_tool_usage(
                        "str_replace_editor", args, "success", "Created missing file"
                    )
                    return f"Observed output of cmd `str_replace_editor`: File '{path_str}' did not exist and was created empty."
                except Exception as e:
                    logger.error(f"Failed to create missing file {target_path}: {e}")
                    self.tool_tracker.record_tool_usage(
                        "str_replace_editor",
                        args,
                        "failure",
                        f"Failed to create missing file: {e}",
                    )
                    return f"Error executing str_replace_editor: Failed to create file '{path_str}'. {str(e)}"

            # Execute original command via base class
            result_str = await super().execute_tool(command)
            # Tool tracker recording handled in the main execute_tool method based on result_str

            return result_str

        except json.JSONDecodeError:
            error_msg = (
                f"Error executing str_replace_editor: Invalid JSON args: {args_str}"
            )
            logger.error(error_msg)
            self.tool_tracker.record_tool_usage(
                "str_replace_editor", {"arguments": args_str}, "failure", "Invalid JSON"
            )
            return f"Error: {error_msg}"
        except Exception as e:
            logger.error(
                f"Unexpected error in _handle_str_replace_editor: {e}", exc_info=True
            )
            self.tool_tracker.record_tool_usage(
                "str_replace_editor", args, "failure", f"Outer handler error: {str(e)}"
            )
            return f"Error executing str_replace_editor: Unexpected error. {str(e)}"

    async def _handle_python_execute(self, command: ToolCall) -> str:
        """Special handler for python_execute: adds error handling wrapper."""
        args = {}
        args_str = command.function.arguments or "{}"
        try:
            args = json.loads(args_str)
            code = args.get("code", "")
            if not code:
                return "Error executing python_execute: Missing 'code' parameter"

            logger.info(f"Preparing python_execute code:\n```python\n{code}\n```")

            # --- Add robust error handling wrapper ---
            if not code.strip().startswith(
                ("try:", "import traceback", "async def")
            ):  # Avoid double-wrapping
                indented_code = "\n".join(
                    ["    " + line for line in code.strip().splitlines()]
                )
                # Use import traceback inside the executed code for better context
                wrapped_code = f"""
import traceback
import sys
try:
{indented_code}
except Exception as e:
    exc_type, exc_value, exc_traceback = sys.exc_info()
    tb_string = "".join(traceback.format_exception(exc_type, exc_value, exc_traceback))
    print(f"--- Python Code Execution Error ---")
    print(f"Error Type: {{type(e).__name__}}")
    print(f"Error Message: {{str(e)}}")
    print(f"Traceback:\\n{{tb_string}}")
    print(f"--- End Error ---", file=sys.stderr) # Print error to stderr
"""
                args["code"] = wrapped_code
                logger.info("Applied error handling wrapper to Python code.")
                command.function.arguments = json.dumps(
                    args
                )  # Update the command object

            # --- Execute via Base Class ---
            logger.info(f"Executing python_execute with final code...")
            result_str = await super().execute_tool(command)
            # Tool tracker recording handled in the main execute_tool method

            return result_str

        except json.JSONDecodeError:
            error_msg = f"Error executing python_execute: Invalid JSON args: {args_str}"
            logger.error(error_msg)
            self.tool_tracker.record_tool_usage(
                "python_execute", {"arguments": args_str}, "failure", "Invalid JSON"
            )
            return f"Error: {error_msg}"
        except Exception as e:
            logger.error(
                f"Unexpected error in _handle_python_execute: {e}", exc_info=True
            )
            arg_info = {"arguments": args_str}
            self.tool_tracker.record_tool_usage(
                "python_execute", arg_info, "failure", f"Outer handler error: {str(e)}"
            )
            return f"Error handling python_execute: Unexpected error. {str(e)}"

    async def _suggest_better_tool(
        self, name: str, args: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """Suggests a more appropriate tool if ask_human is used for solvable tasks."""
        if name != "ask_human":
            return None
        question = args.get("inquire", "")
        q_lower = question.lower()
        if not question:
            return None
        logger.info(f"Analyzing question for better tool: '{question}'")

        # 1. System/Environment -> python_execute
        system_keywords = [
            "current directory",
            "working directory",
            "pwd",
            "list files",
            "environment variable",
            "system info",
            "python version",
        ]
        if any(kw in q_lower for kw in system_keywords):
            # Specific commands based on keywords
            if (
                "current directory" in q_lower
                or "working directory" in q_lower
                or q_lower == "pwd"
            ):
                code = "import os\nprint(f'{os.getcwd()}')"
            elif "list files" in q_lower:
                code = "import os\nprint(f'{os.listdir()}')"
            elif "environment variable" in q_lower:
                match = re.search(r"variable\s+(['\"]?)([A-Z_]\w*)\1", question)
                var = match.group(2) if match else "PATH"
                code = f"import os\nprint(os.environ.get('{var}', 'Not Set'))"
            else:
                code = "import platform, sys\nprint(f'OS: {platform.system()} {platform.release()}\\nPython: {sys.version}')"
            logger.info("Suggestion: Use python_execute for system question.")
            return {"tool": "python_execute", "args": {"code": code}}

        # 2. URL Interaction -> browser_use
        urls = self._extract_urls(question)
        if urls:
            logger.info(f"Suggestion: Use browser_use for URL: {urls[0]}")
            return {
                "tool": "browser_use",
                "args": {"action": "go_to_url", "url": urls[0]},
            }

        # 3. Research/Information -> perform_web_research
        research_keywords = [
            "what is",
            "who is",
            "tell me about",
            "explain",
            "define",
            "history of",
            "how does",
            "why is",
            "compare",
            "pros cons",
            "find info on",
            "news on",
            "statistics for",
        ]
        if (
            q_lower.endswith("?")
            and len(question) > 15
            and any(kw in q_lower for kw in research_keywords)
        ):
            logger.info("Suggestion: Use perform_web_research.")
            return {"tool": "perform_web_research", "args": {"query": question}}

        # 4. File Operations -> str_replace_editor or python_execute
        file_keywords = [
            "read file",
            "file content",
            "edit file",
            "save file",
            "create file",
            "write file",
        ]
        if any(kw in q_lower for kw in file_keywords):
            match = re.search(r"file\s+(['\"]?)([\w\./\-\_]+\.\w+)\1", question)
            filename = match.group(2) if match else None
            if filename:
                if "read file" in q_lower or "content of file" in q_lower:
                    logger.info("Suggestion: Use str_replace_editor (view).")
                    return {
                        "tool": "str_replace_editor",
                        "args": {"command": "view", "path": filename},
                    }
                else:  # For write/edit, let LLM decide exact action
                    logger.info(
                        "Hint: File operation detected. Consider str_replace_editor or python_execute."
                    )
            else:
                logger.info(
                    "Hint: File operation detected but no filename extracted. Consider str_replace_editor or python_execute."
                )
            # Don't force tool for write/edit, just hint

        logger.info("No specific better tool identified. Allowing ask_human.")
        return None

    def add_to_context(
        self,
        text: str,
        priority: str = "medium",
        source: str = "agent",
        tags: List[str] = None,
    ) -> None:
        """Add text to persistent memory for agent context."""
        if tags is None:
            tags = ["context"]
        elif not isinstance(tags, list):
            tags = [str(tags)]

        if priority == "high" or (text and len(text) > 10 and not text.isspace()):
            try:
                self.conversation_memory.store_memory(
                    text=text,
                    source=source,
                    priority=priority,
                    tags=list(set(tags)),  # Deduplicate tags
                    metadata={"timestamp": time.time()},
                )
                logger.debug(
                    f"Stored context (src: {source}, tags: {tags}): {text[:100]}..."
                )
            except Exception as e:
                logger.error(
                    f"Failed to store context in persistent memory: {e}", exc_info=True
                )

    @property
    def context_manager(self):
        """Provides access to PersistentMemory instance for context operations."""
        if not self.conversation_memory:
            logger.error(
                "Context manager accessed before PersistentMemory initialized!"
            )
            # Attempt recovery - this might be too late depending on usage context
            self.initialize_helper()
            if not self.conversation_memory:
                raise RuntimeError("PersistentMemory not initialized.")
        return self.conversation_memory

    async def think(self) -> bool:
        """Enhanced thinking process with dynamic prompting and context awareness."""
        logger.info(f"--- Starting think cycle (Step {self.step_count + 1}) ---")
        self.step_count += 1  # Increment step counter

        # Ensure agent is initialized (connects to MCP etc. on first think)
        if not self._initialized:
            await self.initialize_mcp_servers()

        # --- Pre-computation & State Checks ---
        # 1. Task Analysis on first user message
        is_new_task_interaction = len(self.memory.messages) <= 2
        if is_new_task_interaction and not self.current_task:
            user_message = next(
                (msg for msg in self.memory.messages if msg.role == "user"), None
            )
            if user_message and user_message.content:
                await self.analyze_new_task(user_message.content)

        # 2. Check for Task Completion Readiness
        if (
            self.task_completer
            and self.task_completer.is_ready_to_complete()
            and not self.task_completed
        ):
            logger.info("âœ… Task completion criteria met. Generating deliverable...")
            try:
                self._add_default_information()  # Ensure defaults filled
                deliverable = self.task_completer.create_deliverable()
                file_path = self._save_deliverable_to_file(deliverable)
                save_msg = (
                    f"\nDeliverable saved to workspace: {file_path}"
                    if file_path
                    else ""
                )
                final_output = f"Task '{self.current_task}' completed.\n\nDeliverable:\n{deliverable}{save_msg}"

                # Update memory and context, then mark completed
                self.update_memory("assistant", final_output)
                self.add_to_context(
                    final_output,
                    priority="high",
                    source="task_completion",
                    tags=["task_completed", "deliverable"],
                )
                self.task_completed = True
                # Decide whether to terminate automatically or let LLM call Terminate
                # For now, let LLM decide based on the completion message
                # return True # Returning True might short-circuit LLM call
                logger.info(
                    "Task marked as completed. LLM will decide next step (e.g., Terminate)."
                )
            except Exception as e:
                logger.error(f"Error during task completion/saving: {e}", exc_info=True)
                self.add_to_context(
                    f"Error finalizing task: {e}",
                    priority="high",
                    source="error",
                    tags=["error", "task_completion"],
                )
                # Continue thinking cycle to report error

        # --- Dynamic Prompt Construction ---
        base_prompt = self.next_step_prompt
        prompt_prefix = ""

        # 1. Browser Context / Hints
        active_url = (
            self.browser_context_helper.get_current_url()
            if self.browser_context_helper
            else None
        )
        if active_url and self.browser_used:
            browser_ctx = (
                await self.browser_context_helper.format_next_step_prompt()
            )  # Viewport info
            prompt_prefix += browser_ctx
            # Navigation suggestion
            next_nav = self.browser_navigator.suggest_browser_action(
                active_url, self.current_task
            )
            if next_nav and next_nav.get("tool") == "browser_use":
                s_url = next_nav["args"].get("url")
                if s_url and s_url != active_url and s_url not in self.visited_urls:
                    prompt_prefix += (
                        f"\nNavigation Suggestion: Consider visiting {s_url} next.\n"
                    )
        elif not self.browser_used and self.mentioned_urls:
            url_to_check = self._get_most_relevant_url(self.current_task)
            if url_to_check and url_to_check not in self.visited_urls:
                prompt_prefix += f"\nHint: Task mentions URLs like {url_to_check}. Use 'browser_use' to visit it.\n"

        # 2. Relevant Memories
        try:
            # Use semantic search (assuming sync for simplicity, adjust if async)
            relevant_memories = self.conversation_memory.search_memories_semantic(
                self.current_task, limit=4, threshold=0.7
            )
            if relevant_memories:
                mem_items = [
                    f"- {mem.text[:150]}... (src: {mem.source}, score: {score:.2f})"
                    for mem, score in relevant_memories
                ]
                prompt_prefix += (
                    "\n--- Relevant Info from Memory ---\n"
                    + "\n".join(mem_items)
                    + "\n---\n"
                )
        except Exception as e:
            logger.error(f"Error retrieving relevant memories: {e}")

        # 3. SmartAskHuman Context (Answered Questions)
        smart_ask_tool = self.available_tools.get_tool("ask_human")
        if isinstance(smart_ask_tool, SmartAskHuman):
            answered = (
                smart_ask_tool.get_answered_questions()
            )  # Get dict {question: answer}
            if answered:
                prompt_prefix += "\n--- Previously Answered (Do NOT ask again) ---\n"
                for q, a in answered.items():
                    prompt_prefix += f"Q: {q} -> A: {a[:80]}...\n"
                prompt_prefix += "---\n"

        # 4. Task Completion Reminder (if applicable)
        # Example: Remind after N steps if task not completed
        if (
            not self.task_completed
            and self.step_count > 5
            and self.task_completer
            and not self.task_completer.is_ready_to_complete()
        ):
            missing = self.task_completer.get_missing_info()
            if missing:
                prompt_prefix += f"\nReminder: Still need info for: {', '.join(missing)}. Focus on gathering this or completing the task.\n"
            else:
                prompt_prefix += "\nReminder: You seem to have enough info. Aim to synthesize and complete the task soon.\n"

        # Combine prompt parts
        final_prompt = f"{prompt_prefix}\n{base_prompt}"

        # --- Execute Superclass Think ---
        original_prompt = self.next_step_prompt
        self.next_step_prompt = final_prompt
        logger.debug(f"Using combined next_step_prompt:\n{self.next_step_prompt}")
        try:
            result = (
                await super().think()
            )  # Let ToolCallAgent handle LLM call and tool parsing
        finally:
            self.next_step_prompt = original_prompt  # Restore base prompt

        logger.info(f"--- Finished think cycle (Step {self.step_count}) ---")
        return result

    def _add_default_information(self) -> None:
        """Add default or inferred information if task completer requires it."""
        if not self.task_completer:
            return
        required = self.task_completer.get_missing_info()
        if not required:
            return

        logger.info(f"Attempting to add default/inferred info for: {required}")
        # Example: Infer product name for marketing plan
        if (
            "marketing_plan" in self.task_completer.task_type.lower()
            and "product_name" in required
        ):
            match = re.search(
                r"(?:marketing plan|plan) for ([\w\s\-]+)", self.current_task.lower()
            )
            name = match.group(1).strip() if match else "the specified product/service"
            self.task_completer.add_information("product_name", name)
            logger.info(f"Inferred product_name: {name}")

        # Add more logic here based on task types and common missing info

    def _save_deliverable_to_file(self, deliverable_content: str) -> Optional[str]:
        """Saves deliverable content to a structured file in the workspace."""
        if not deliverable_content:
            return None
        try:
            workspace_root = Path(config.workspace_root)
            deliverables_dir = workspace_root / "deliverables"
            task_slug = _slugify(
                self.current_task or self.task_completer.task_type or "task"
            )[:50]
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            task_folder = deliverables_dir / f"{task_slug}_{timestamp}"
            task_folder.mkdir(parents=True, exist_ok=True)

            ext = ".md"  # Default markdown
            task_type_lower = (self.task_completer.task_type or "").lower()
            if "code" in task_type_lower:
                ext = ".py"  # Simple guess
            elif "report" in task_type_lower:
                ext = ".txt"
            elif "plan" in task_type_lower:
                ext = ".md"

            file_name = (
                f"{_slugify(self.task_completer.task_type or 'deliverable')}{ext}"
            )
            deliverable_path = task_folder / file_name

            with open(deliverable_path, "w", encoding="utf-8") as f:
                f.write(deliverable_content)
            relative_path = str(deliverable_path.relative_to(workspace_root))
            logger.info(f"âœ… Deliverable saved to: {relative_path}")
            return relative_path
        except Exception as e:
            logger.error(f"Error saving deliverable: {e}", exc_info=True)
            return None

    def update_memory(self, role: str, content: str) -> None:
        """Adds a message to the main conversation memory (part of ToolCallAgent)."""
        self.memory.add_message(Message(role=role, content=content))
        logger.debug(f"Updated main memory ({role}): {content[:100]}...")

    # Included previously necessary helpers like _process_content_for_task etc. if needed by the above logic
    # Need to ensure all called helpers are defined.
    def _extract_section(self, content: str, section_name: str) -> str:
        """Placeholder: Extracts a specific section based on keywords."""
        # This needs a more robust implementation (e.g., using heading tags)
        pattern = re.compile(
            rf"(?:^|\n)[#\s]*{re.escape(section_name)}[^\n]*\n(.*?)(?:\n[#\s]*\w+|\Z)",
            re.IGNORECASE | re.DOTALL,
        )
        match = pattern.search(content)
        if match:
            return match.group(1).strip()[:1000]  # Limit length
        # Fallback: find first mention
        idx = content.lower().find(section_name.lower())
        if idx != -1:
            return content[idx : idx + 500].strip() + "..."
        return f"(Section '{section_name}' not clearly identified)"

    def _process_content_for_task(self, url: str, content: str) -> None:
        """Process extracted web content to fill TaskCompleter info."""
        if not self.task_completer:
            return

        logger.debug(
            f"Processing content from {url} for task type: {self.task_completer.task_type}"
        )
        task_type = (self.task_completer.task_type or "").lower()

        # Example for Marketing Plan (add more types and refine logic)
        if "marketing_plan" in task_type:
            required = self.task_completer.get_missing_info()
            if not required:
                return  # Don't process if already complete

            # Simple keyword-based extraction for example
            if "product_name" in required:
                # Try to find product name in title or h1
                soup = BeautifulSoup(content[:5000], "html.parser")  # Parse snippet
                title = soup.title.string if soup.title else ""
                h1 = soup.h1.string if soup.h1 else ""
                # Basic logic, needs improvement
                if title and len(title) < 50:
                    self.task_completer.add_information("product_name", title.strip())
                elif h1 and len(h1) < 50:
                    self.task_completer.add_information("product_name", h1.strip())

            sections_keywords = {
                "target_audience": ["target audience", "who is it for", "customers"],
                "value_proposition": ["value proposition", "benefits", "why choose"],
                "features": ["features", "capabilities", "what it does"],
                # Add more sections
            }
            for section_key, keywords in sections_keywords.items():
                if section_key in required:
                    # Try to find section using keywords (crude example)
                    for kw in keywords:
                        if kw in content.lower():
                            extracted = self._extract_section(content, kw)  # Use helper
                            self.task_completer.add_information(section_key, extracted)
                            logger.info(
                                f"Extracted '{section_key}' based on keyword '{kw}'"
                            )
                            break  # Stop after first keyword match for this section


# --- Module Level Utility ---
import asyncio  # Ensure asyncio is imported if used standalone


def _slugify(text):
    """Convert text into a filesystem-friendly slug."""
    text = str(text).lower()
    text = re.sub(r"&", "-and-", text)  # Replace &
    text = re.sub(r"[^a-z0-9\-\_]+", "-", text)  # Replace non-alphanumeric by -
    text = re.sub(r"-+", "-", text)  # Collapse multiple dashes
    text = text.strip("-")  # Remove leading/trailing dashes
    return text or "task"  # Return 'task' if empty


# Required for MCP connection logic if run standalone/tested directly
import asyncio
